import os
import json
import requests
import random
from datetime import datetime

from pyspark.sql import SparkSession
from pyspark.sql.functions import (
    col, from_json, to_timestamp, window, collect_list, struct
)
from pyspark.sql.types import (
    StructType, StructField,
    StringType, DoubleType
)

from kafka import KafkaProducer
from db.alert_repository import save_alert, update_alert_sent


###########################################################
# CONFIG
###########################################################
def load_config():
    return {
        "KAFKA_BOOTSTRAP": os.getenv("KAFKA_BOOTSTRAP"),
        "TOPIC_NAME": os.getenv("TOPIC_WEATHER"),
        "RETRY_TOPIC": os.getenv("TOPIC_RETRY"),
        "SLACK_WEBHOOK_URL": os.getenv("SLACK_WEBHOOK_URL"),

        "WINDOW_SECONDS": int(os.getenv("WINDOW_SECONDS", "60")),
        "ALERT_INTERVAL_MINUTES": int(os.getenv("ALERT_INTERVAL_MINUTES", "30")),
        "WATER_MARK_MINUTES": int(os.getenv("WATERMARK_MINUTES", "2")),
        "HIGH_TEMPERATURE_THRESHOLD": float(os.getenv("HIGH_TEMPERATURE_THRESHOLD", "35")),
        "LOW_TEMPERATURE_THRESHOLD": float(os.getenv("LOW_TEMPERATURE_THRESHOLD", "0")),
        "RAINFALL_THRESHOLD": float(os.getenv("RAINFALL_THRESHOLD", "30")),
        "WIND_SPEED_THRESHOLD": float(os.getenv("WIND_SPEED_THRESHOLD", "40")),
        "RANDOM_LIMIT": float(os.getenv("RANDOM_LIMIT", "0.2")),
        # ì„±ëŠ¥/ì•ˆì •ì„± ì˜µì…˜ (ì›í•˜ë©´ ENVë¡œ êº¼ë‚¼ ìˆ˜ ìˆìŒ)
        "MAX_OFFSETS_PER_TRIGGER": int(os.getenv("MAX_OFFSETS_PER_TRIGGER", "2000")),
        "CHECKPOINT_LOCATION": os.getenv("CHECKPOINT_LOCATION", "/shared-checkpoints/weather-alert"),
    }


###########################################################
# ALERT STATE (ì¿¨ë‹¤ìš´)
###########################################################
last_alert_time = {}


def should_alert(location, alert_type, event_time, cooldown_min):
    """
    ê°™ì€ (location, alert_type)ì— ëŒ€í•´ ì¼ì • ì‹œê°„ ë‚´ ì¤‘ë³µ ì•Œë¦¼ ë°©ì§€
    """
    key = (location, alert_type)

    if key in last_alert_time:
        diff = (event_time - last_alert_time[key]).total_seconds()
        if diff < cooldown_min * 60:
            return False

    last_alert_time[key] = event_time
    return True


###########################################################
# Slack
###########################################################
def send_slack(webhook_url, payload: str) -> bool:
    try:
        res = requests.post(webhook_url, json={"text": payload}, timeout=5)
        return res.status_code == 200
    except Exception as e:
        print(f"[SLACK] Error: {e}")
        return False


###########################################################
# Schema
###########################################################
def get_schema():
    return StructType([
        StructField("Location", StringType()),
        StructField("Date_Time", StringType()),
        StructField("Temperature_C", DoubleType()),
        StructField("Humidity_pct", DoubleType()),
        StructField("Precipitation_mm", DoubleType()),
        StructField("Wind_Speed_kmh", DoubleType()),
        StructField("retry", DoubleType()),
    ])


###########################################################
# Alert Logic
###########################################################
def detect_alert_types(row, cfg):
    alerts = []

    t = row.get("Temperature_C")
    p = row.get("Precipitation_mm")
    w = row.get("Wind_Speed_kmh")

    if t is not None and t >= cfg["HIGH_TEMPERATURE_THRESHOLD"]:
        alerts.append((
            "TEMP_HIGH",
            f"Temperature {t}Â°C >= {cfg['HIGH_TEMPERATURE_THRESHOLD']}Â°C",
            t,
            cfg["HIGH_TEMPERATURE_THRESHOLD"],
        ))

    if t is not None and t <= cfg["LOW_TEMPERATURE_THRESHOLD"]:
        alerts.append((
            "TEMP_LOW",
            f"Temperature {t}Â°C <= {cfg['LOW_TEMPERATURE_THRESHOLD']}Â°C",
            t,
            cfg["LOW_TEMPERATURE_THRESHOLD"],
        ))

    if p is not None and p >= cfg["RAINFALL_THRESHOLD"]:
        alerts.append((
            "RAIN_HEAVY",
            f"Rainfall {p}mm >= {cfg['RAINFALL_THRESHOLD']}mm",
            p,
            cfg["RAINFALL_THRESHOLD"],
        ))

    if w is not None and w >= cfg["WIND_SPEED_THRESHOLD"]:
        alerts.append((
            "WIND_STRONG",
            f"Wind {w} km/h >= {cfg['WIND_SPEED_THRESHOLD']} km/h",
            w,
            cfg["WIND_SPEED_THRESHOLD"],
        ))

    return alerts


###########################################################
# foreachBatch ì²˜ë¦¬ í•¨ìˆ˜ ìƒì„± (í´ë¡œì €)
###########################################################
def create_batch_processor(cfg, retry_producer):

    def process_window_batch(df, batch_id):
        """
        âš  ì¤‘ìš”: df.collect() ì‚¬ìš©í•˜ì§€ ì•Šê³ , toLocalIterator()ë¡œ ìŠ¤íŠ¸ë¦¬ë°í•˜ê²Œ ì²˜ë¦¬
        """
        print(f"[BATCH {batch_id}] Starting batch processing...")

        # df: columns = [window, Location, rows]
        # rows: array<struct<ì›ë³¸ ì»¬ëŸ¼ë“¤ + event_time>>
        count = 0

        try:
            for grouped_row in df.toLocalIterator():
                loc = grouped_row["Location"]
                window_struct = grouped_row["window"]
                window_start = window_struct["start"]
                window_end = window_struct["end"]
                record_list = grouped_row["rows"]  # ì´ê²Œ ì´ë¯¸ array<struct> (ë°°ì—´)

                for raw_row in record_list:
                    raw_dict = raw_row.asDict(recursive=True)
                    event_time = raw_dict.get("event_time")

                    # event_timeì´ stringì´ë©´ datetimeìœ¼ë¡œ ë³€í™˜
                    if isinstance(event_time, str):
                        try:
                            event_time = datetime.fromisoformat(event_time)
                            raw_dict["event_time"] = event_time
                        except Exception:
                            # íŒŒì‹± ì‹¤íŒ¨ ì‹œ ê·¸ëƒ¥ ìŠ¤í‚µ
                            continue

                    # ê°•ì œ retry ìƒ˜í”Œë§
                    if random.random() < cfg["RANDOM_LIMIT"]:
                        safe_dict = dict(raw_dict)
                        if isinstance(safe_dict.get("event_time"), datetime):
                            safe_dict["event_time"] = safe_dict["event_time"].isoformat()

                        retry_producer.send(
                            cfg["RETRY_TOPIC"],
                            key=loc,
                            value=safe_dict,
                        )
                        print(f"âš  Forced retry â†’ {cfg['RETRY_TOPIC']} / loc={loc}")
                        continue

                    # Alert ì¡°ê±´ ê²€ì‚¬
                    alerts = detect_alert_types(raw_dict, cfg)

                    for alert_type, reason, value, threshold in alerts:
                        if not should_alert(
                            loc,
                            alert_type,
                            event_time,
                            cfg["ALERT_INTERVAL_MINUTES"],
                        ):
                            continue

                        # DB ì €ì¥
                        alert_id = save_alert(
                            location=loc,
                            alert_type=alert_type,
                            alert_reason=reason,
                            event_time=event_time,
                            value=value,
                            threshold=threshold,
                            raw_row=raw_dict,
                            slack_sent=False,
                        )

                        emoji = {
                            "TEMP_HIGH": "ğŸ”¥",
                            "TEMP_LOW": "â„ï¸",
                            "RAIN_HEAVY": "ğŸŒ§ï¸",
                            "WIND_STRONG": "ğŸ’¨",
                        }.get(alert_type, "âš ï¸")

                        payload = (
                            f"{emoji} *{alert_type.replace('_', ' ')} Alert*\n"
                            f"Location: {loc}\n"
                            f"{reason}\n"
                            f"Event Time: {event_time}\n"
                            f"Window: {window_start} ~ {window_end}"
                        )

                        success = send_slack(cfg["SLACK_WEBHOOK_URL"], payload)

                        if success:
                            update_alert_sent(alert_id)
                            print(f"ğŸš¨ Alert sent & updated: loc={loc}, type={alert_type}")
                        else:
                            print(f"âš  Slack failed: loc={loc}, type={alert_type}")

                        count += 1

            print(f"[BATCH {batch_id}] Done. Alerts processed: {count}")

        except Exception as e:
            # ë°°ì¹˜ ì „ì²´ê°€ ì£½ì–´ë„ ìŠ¤íŠ¸ë¦¬ë°ì€ ê³„ì† ëŒì•„ê°€ë„ë¡ ë¡œê·¸ë§Œ ì°ê¸°
            print(f"[BATCH {batch_id}] ERROR: {e}")

    return process_window_batch


###########################################################
# MAIN
###########################################################
def main():
    cfg = load_config()
    print("[INIT] Loaded config:", cfg)

    # Kafka Producer (late init)
    retry_producer = KafkaProducer(
        bootstrap_servers=cfg["KAFKA_BOOTSTRAP"].split(","),
        key_serializer=lambda v: v.encode("utf-8"),
        value_serializer=lambda v: json.dumps(v).encode("utf-8"),
    )

    # Spark Session
    spark = (
        SparkSession.builder
        .appName("WeatherAlertConsumer")
        .master("spark://spark-master:7077")
        .config(
            "spark.jars.packages",
            "org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.6",
        )
        # executor/ë„¤íŠ¸ì›Œí¬ ì•ˆì •ì„± ì˜µì…˜ (ì›í•˜ë©´ ë” ì¶”ê°€)
        .config("spark.executor.heartbeatInterval", "20s")
        .config("spark.network.timeout", "300s")
        .getOrCreate()
    )
    spark.sparkContext.setLogLevel("WARN")

    schema = get_schema()

    # Kafkaì—ì„œ raw read
    raw_df = (
        spark.readStream
        .format("kafka")
        .option("kafka.bootstrap.servers", cfg["KAFKA_BOOTSTRAP"])
        .option("subscribe", cfg["TOPIC_NAME"])
        .option("startingOffsets", "latest")
        .option("kafka.request.timeout.ms", "60000")
        .option("kafka.session.timeout.ms", "60000")
        .option("kafka.connection.timeout.ms", "60000")
        .option("maxOffsetsPerTrigger", cfg["MAX_OFFSETS_PER_TRIGGER"])
        .load()
    )

    # JSON íŒŒì‹± + event_time ì»¬ëŸ¼ ì¶”ê°€
    parsed_df = (
        raw_df.selectExpr("CAST(value AS STRING)")
        .select(from_json(col("value"), schema).alias("data"))
        .select("data.*")
        .withColumn("event_time", to_timestamp(col("Date_Time")))
    )

    # ìœˆë„ìš° + ì›Œí„°ë§ˆí¬
    windowed_df = (
        parsed_df
        .withWatermark("event_time", f"{cfg['WATER_MARK_MINUTES']} minutes")
        .groupBy(
            window(col("event_time"), f"{cfg['WINDOW_SECONDS']} seconds"),
            col("Location"),
        )
        .agg(collect_list(struct("*")).alias("rows"))
    )

    process_fn = create_batch_processor(cfg, retry_producer)

    query = (
        windowed_df.writeStream
        .foreachBatch(process_fn)
        .outputMode("update")
        .option("checkpointLocation", cfg["CHECKPOINT_LOCATION"])
        .start()
    )

    print("[STREAM] WeatherAlertConsumer started.")
    spark.streams.awaitAnyTermination()


###########################################################
# ENTRYPOINT
###########################################################
if __name__ == "__main__":
    main()
