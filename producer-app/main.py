import os
import csv
import json
import time
from kafka import KafkaProducer
from watchdog.observers.polling import PollingObserver
from watchdog.events import FileSystemEventHandler
from kafka.errors import NoBrokersAvailable

# ============================================================
# í™˜ê²½ë³€ìˆ˜ ê¸°ë°˜ ì„¤ì •
# ============================================================
WATCH_DIR = os.getenv("WATCH_DIR", "/app/data/origin-data")
TOPIC_NAME = os.getenv("TOPIC_WEATHER", "weather-data")
ERROR_TOPIC = os.getenv("TOPIC_ERROR", "error-data")

BOOTSTRAP_SERVERS = os.getenv("KAFKA_BOOTSTRAP", "kafka-1:9092,kafka-2:9092,kafka-3:9092")

MAP_FILE_PATH = os.getenv("REGION_MAP_FILE", "/app/region_partition_map.json")
NUM_PARTITIONS = int(os.getenv("NUM_PARTITIONS", "10"))

# íŒŒì¼ ë‹¨ìœ„ ì²´í¬í¬ì¸íŠ¸ ì €ì¥ íŒŒì¼
CHECKPOINT_FILE = os.getenv("PRODUCER_CHECKPOINT_FILE", "/app/producer_checkpoint.json")

region_partition_map = {}
processed_files = set()  # ì´ë¯¸ ì²˜ë¦¬ ì™„ë£Œí•œ íŒŒì¼ ì´ë¦„(basename) ì €ì¥

# í•„ìˆ˜ ì»¬ëŸ¼ ì •ì˜
REQUIRED_FIELDS = [
    "Location", "Date_Time",
    "Temperature_C", "Humidity_pct",
    "Precipitation_mm", "Wind_Speed_kmh"
]

# ìˆ«ìë¡œ ë³€í™˜í•´ì•¼ í•˜ëŠ” í•„ë“œ ë¦¬ìŠ¤íŠ¸
NUMERIC_FIELDS_FLOAT = [
    "Temperature_C",
    "Humidity_pct",
    "Precipitation_mm",
    "Wind_Speed_kmh"
]
NUMERIC_FIELDS_INT = ["retry"]


# ============================================================
# ìˆ«ì ìë™ ë³€í™˜
# ============================================================
def convert_numeric_fields(row):
    new_row = dict(row)

    for f in NUMERIC_FIELDS_FLOAT:
        if f in new_row:
            try:
                new_row[f] = float(new_row[f])
            except Exception:
                return None, f"Invalid float field: {f}='{new_row[f]}'"

    for f in NUMERIC_FIELDS_INT:
        if f in new_row:
            try:
                new_row[f] = int(new_row[f])
            except Exception:
                new_row[f] = 0  # fallback

    return new_row, None


# ============================================================
# ìŠ¤í‚¤ë§ˆ ê²€ì¦
# ============================================================
def validate_row(row):
    # 1) í•„ìˆ˜ í•„ë“œ ì²´í¬
    for f in REQUIRED_FIELDS:
        if f not in row or row[f] == "":
            return False, "MISSING_FIELD", f"Missing required field: {f}"

    # 2) ìˆ«ì ë³€í™˜
    converted, err = convert_numeric_fields(row)
    if err:
        return False, "TYPE_ERROR", err

    return True, "OK", converted


# ============================================================
# JSON ë¡œë“œ & ì €ì¥ (ì§€ì—­â†’íŒŒí‹°ì…˜)
# ============================================================
def load_region_map():
    global region_partition_map
    
    # íŒŒì¼ ì—†ìœ¼ë©´ ë¹ˆ íŒŒì¼ ìƒì„±
    if not os.path.exists(MAP_FILE_PATH):
        print(f"ğŸ†• No region map found. Creating empty map at {MAP_FILE_PATH}")
        region_partition_map = {}
        
        # ë°”ë¡œ ìƒì„± (ë¹ˆ JSON êµ¬ì¡°)
        try:
            with open(MAP_FILE_PATH, "w") as f:
                json.dump(region_partition_map, f, indent=2)
        except Exception as e:
            print(f"âš ï¸ Could not create empty map file: {e}")
        return

    # íŒŒì¼ì´ ì¡´ì¬í•˜ë©´ ì½ê¸°
    try:
        with open(MAP_FILE_PATH, "r") as f:
            region_partition_map = json.load(f)
        print(f"ğŸ” Loaded region map: {region_partition_map}")
    except Exception as e:
        print(f"âš ï¸ Failed to load region map. Resetting it. Error: {e}")
        region_partition_map = {}


def save_region_map():
    try:
        with open(MAP_FILE_PATH, "w") as f:
            json.dump(region_partition_map, f, indent=2)
        print(f"ğŸ’¾ Region map saved: {region_partition_map}")
    except Exception as e:
        print(f"âš ï¸ Failed to save region map: {e}")


# ============================================================
# CSV ì½ê¸°
# ============================================================
def read_csv_file(path):
    with open(path, "r", encoding="utf-8") as f:
        return list(csv.DictReader(f))


# ============================================================
# íŒŒí‹°ì…˜ ê²°ì •
# ============================================================
def get_partition_for_region(region):
    if region not in region_partition_map:
        region_partition_map[region] = len(region_partition_map) % NUM_PARTITIONS
        save_region_map()
    return region_partition_map[region]


# ============================================================
# ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ & ì €ì¥ (íŒŒì¼ ë‹¨ìœ„)
# ============================================================
def load_checkpoint():
    global processed_files
    if os.path.exists(CHECKPOINT_FILE):
        try:
            with open(CHECKPOINT_FILE, "r") as f:
                data = json.load(f)
            processed_files = set(data.get("processed_files", []))
            print(f"ğŸ“‚ Loaded checkpoint. processed_files={len(processed_files)}")
        except Exception as e:
            print(f"âš ï¸ Failed to load checkpoint: {e}")
            processed_files = set()
    else:
        processed_files = set()
        print("ğŸ†• No existing checkpoint file.")


def save_checkpoint():
    try:
        with open(CHECKPOINT_FILE, "w") as f:
            json.dump({"processed_files": list(processed_files)}, f, indent=2)
    except Exception as e:
        print(f"âš ï¸ Failed to save checkpoint: {e}")


def mark_file_processed(path):
    """íŒŒì¼ ì²˜ë¦¬ê°€ ëë‚¬ì„ ë•Œ basename ê¸°ì¤€ìœ¼ë¡œ ê¸°ë¡"""
    filename = os.path.basename(path)
    processed_files.add(filename)
    save_checkpoint()


def is_file_processed(path):
    filename = os.path.basename(path)
    return filename in processed_files


# ============================================================
# íŒŒì¼ í•˜ë‚˜ ì²˜ë¦¬ (ì¬ì‚¬ìš© ê°€ëŠ¥í•˜ë„ë¡ í•¨ìˆ˜ë¡œ ë¶„ë¦¬)
# ============================================================
def process_file(path, producer):
    """ë‹¨ì¼ CSV íŒŒì¼ ì „ì²´ë¥¼ ì½ì–´ì„œ Kafkaë¡œ ì „ì†¡"""
    filename = os.path.basename(path)

    if is_file_processed(path):
        print(f"â­  Skip already processed file: {filename}")
        return

    print(f"ğŸ“¥ Processing file: {filename}")

    try:
        rows = read_csv_file(path)
        success_count = 0
        error_count = 0

        for row in rows:
            row["retry"] = 0
            location = row.get("Location") or "unknown"

            ok, error_type, result = validate_row(row)

            if not ok:
                error_data = {
                    "error_type": error_type,
                    "error_reason": result,
                    "raw_row": row,
                    "timestamp": time.time(),
                    "file_name": filename,
                    "retry_count": 0
                }

                producer.send(
                    ERROR_TOPIC,
                    key=location.encode(),
                    value=error_data
                )
                error_count += 1
                continue

            valid_row = result
            partition = get_partition_for_region(location)

            producer.send(
                TOPIC_NAME,
                key=location.encode(),
                value=valid_row,
                partition=partition
            )
            success_count += 1

        producer.flush()
        mark_file_processed(path)
        print(f"âœ… Sent {success_count} rows from {filename} (errors={error_count})")

    except Exception as e:
        print(f"âŒ Error processing {filename}: {e}")


# ============================================================
# íŒŒì¼ ìƒì„± ì´ë²¤íŠ¸ í•¸ë“¤ëŸ¬
# ============================================================
class NewFileHandler(FileSystemEventHandler):
    def __init__(self, producer):
        self.producer = producer

    def on_created(self, event):
        if event.is_directory or not event.src_path.endswith(".csv"):
            return
        # ìƒˆë¡œ ìƒì„±ëœ íŒŒì¼ ì²˜ë¦¬
        process_file(event.src_path, self.producer)


# ============================================================
# Kafka ì—°ê²°
# ============================================================
def connect_kafka():
    for i in range(20):
        try:
            return KafkaProducer(
                bootstrap_servers=BOOTSTRAP_SERVERS.split(","),
                key_serializer=lambda k: k,
                value_serializer=lambda v: json.dumps(v).encode("utf-8")
            )
        except NoBrokersAvailable:
            print(f"[WARN] Kafka not ready... retry {i+1}/20")
            time.sleep(3)
    raise Exception("Kafka not available after retries")


# ============================================================
# ë©”ì¸ ì‹¤í–‰
# ============================================================
def main():
    os.makedirs(WATCH_DIR, exist_ok=True)

    load_region_map()
    load_checkpoint()

    producer = connect_kafka()

    # 1) ì‹œì‘ ì‹œ ê¸°ì¡´ íŒŒì¼ë“¤ ë¨¼ì € ì²˜ë¦¬
    existing_files = sorted(
        f for f in os.listdir(WATCH_DIR)
        if f.endswith(".csv")
    )

    print(f"ğŸ” Found {len(existing_files)} existing CSV files at startup.")

    for fname in existing_files:
        full_path = os.path.join(WATCH_DIR, fname)
        process_file(full_path, producer)

    # 2) ì´í›„ ìƒˆë¡œ ìƒì„±ë˜ëŠ” íŒŒì¼ ê°ì‹œ
    event_handler = NewFileHandler(producer)
    observer = PollingObserver(timeout=1.0)
    observer.schedule(event_handler, WATCH_DIR, recursive=False)
    observer.start()

    print(f"ğŸ‘€ Watching directory (polling): {WATCH_DIR}")

    try:
        while True:
            time.sleep(1)
    except KeyboardInterrupt:
        observer.stop()

    observer.join()


if __name__ == "__main__":
    main()
