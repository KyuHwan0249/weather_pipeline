#!/bin/bash
set -e

echo "ðŸ›‘ Stopping all containers..."
docker compose down -v

echo ""
echo "ðŸ§± Generating cluster ID..."
CLUSTER_ID=$(uuidgen)
echo "ðŸ“Œ CLUSTER_ID = $CLUSTER_ID"

############################################################
# FUNCTION: SAFE CLEAR LOCAL DIRECTORIES
############################################################
safe_clear() {
  TARGET=$1
  echo "ðŸ—‘ Clearing $TARGET ..."
  docker run --rm -v $TARGET:/data busybox sh -c \
    "find /data -mindepth 1 ! -name '.gitkeep' -exec rm -rf {} +"
}

############################################################
# 1) REMOVE OLD DOCKER VOLUMES
############################################################
echo ""
echo "ðŸ—‘ Removing old Kafka volumes..."
docker volume rm -f weather_pipeline_kafka-1-data || true
docker volume rm -f weather_pipeline_kafka-2-data || true
docker volume rm -f weather_pipeline_kafka-3-data || true

echo ""
echo "ðŸ—‘ Removing old Spark & Grafana volumes..."
docker volume rm -f weather_pipeline_spark-checkpoints || true
docker volume rm -f weather_pipeline_grafana-data || true

############################################################
# 2) CREATE NEW VOLUMES
############################################################
echo ""
echo "ðŸ“¦ Creating fresh Kafka volumes..."
docker volume create weather_pipeline_kafka-1-data
docker volume create weather_pipeline_kafka-2-data
docker volume create weather_pipeline_kafka-3-data

echo "ðŸ“¦ Creating Spark checkpoint volume..."
docker volume create weather_pipeline_spark-checkpoints

echo "ðŸ“¦ Creating Grafana data volume..."
docker volume create weather_pipeline_grafana-data

############################################################
# 3) FORMAT KAFKA STORAGE
############################################################
echo ""
echo "âš™ Formatting Kafka storage..."

format_kafka() {
  BROKER_NAME=$1
  PROP_FILE=$2
  VOLUME_NAME=$3

  echo "ðŸ“Œ Formatting $BROKER_NAME ..."

  docker run --rm \
    -v $VOLUME_NAME:/var/lib/kafka/data \
    -v $(pwd)/kafka/$PROP_FILE:/opt/kafka/config/kraft/server.properties \
    apache/kafka:3.7.0 \
    bash -c "/opt/kafka/bin/kafka-storage.sh format \
      -t $CLUSTER_ID \
      -c /opt/kafka/config/kraft/server.properties \
      --ignore-formatted"
}

format_kafka "broker1" "server-1.properties" "weather_pipeline_kafka-1-data"
format_kafka "broker2" "server-2.properties" "weather_pipeline_kafka-2-data"
format_kafka "broker3" "server-3.properties" "weather_pipeline_kafka-3-data"


############################################################
# 4) CLEAR LOCAL PROJECT FOLDERS
############################################################

echo ""
echo "ðŸ—‘ Clearing MinIO data..."
safe_clear "$(pwd)/minio-data"

echo "ðŸ—‘ Clearing Postgres data..."
safe_clear "$(pwd)/pgdata"

echo "ðŸ—‘ Clearing Spark checkpoints..."
safe_clear "$(pwd)/spark-checkpoints"

echo "ðŸ—‘ Clearing Origin data..."
safe_clear "$(pwd)/origin-data"

echo "ðŸ—‘ Clearing Airflow logs..."
safe_clear "$(pwd)/airflow/logs"

echo "ðŸ—‘ Clearing Producer checkpoints..."
rm -f producer-app/producer_checkpoint.json || true
rm -f producer-app/region_partition_map.json || true

echo "ðŸ—‘ Clearing Origin Generator state..."
safe_clear "$(pwd)/create-origin-data-app/state"

echo "ðŸ—‘ Clearing Origin output files..."
rm -f origin-data/* || true

############################################################
# FINISH
############################################################

echo ""
echo "ðŸŽ‰ Reset complete!"
echo "â–¶ Run: docker compose up -d"
echo "ðŸ§© Kafka cluster formatted with CLUSTER_ID = $CLUSTER_ID"
